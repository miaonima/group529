{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from sklearn import model_selection, linear_model, metrics, pipeline, preprocessing\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and separate the id's\n",
    "X_data = pd.read_hdf(\"cat.hdf5\", \"train\")\n",
    "y_data = pd.read_hdf(\"cat.hdf5\", \"train_target\")\n",
    "X_test = pd.read_hdf(\"cat.hdf5\", \"test\")\n",
    "\n",
    "data_id = X_data.loc[:, \"id\"]\n",
    "test_id = X_test.loc[:, \"id\"]\n",
    "\n",
    "X_data.drop(columns=\"id\", inplace=True)\n",
    "X_test.drop(columns=\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data with labels into training and validation.\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X_data, y_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210000, 40) (210000,) (90000, 40) (90000,) (200000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression as baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"min_max_scaler\", preprocessing.MinMaxScaler()),\n",
    "        (\"logistic_classifier\", linear_model.LogisticRegressionCV(\n",
    "            solver=\"lbfgs\", max_iter=2000, cv=5, n_jobs=-1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "logistic.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = logistic.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7663509689077292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model):\n",
    "    y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "    val_score = metrics.roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "    y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "    train_score = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "\n",
    "    return (train_score, val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7650231180301985, 0.7663509689077292)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not use GridSearchCV:\n",
    "# See https://github.com/dmlc/xgboost/issues/2819\n",
    "# Instead we implement our own grid search.\n",
    "\n",
    "from collections.abc import Iterable\n",
    "import itertools\n",
    "\n",
    "# Takes a parameter grid similar to the format in\n",
    "# sklearn GridSearchCV and returns a list of names every time.\n",
    "def get_grid_iter(param_grid):\n",
    "    names = []\n",
    "    values = []\n",
    "    \n",
    "    for param_name, param_values in param_grid.items():\n",
    "        names.append(param_name)\n",
    "        \n",
    "        if isinstance(param_values, list):\n",
    "            values.append(list(param_values))\n",
    "        else:\n",
    "            values.append(list([param_values]))\n",
    "    \n",
    "    it = (dict(zip(names, param)) for param in itertools.product(*values))\n",
    "    item_cnt = np.prod(np.array([len(v) for v in values]))\n",
    "    return it, item_cnt\n",
    "\n",
    "# Similar to GridSearchCV\n",
    "def hyper_opt(model_base, param_grid, metric, X_train, y_train, X_val, y_val, bin_prob, verbose=False):\n",
    "    best_param = None\n",
    "    best_param_id = -1\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    it, item_cnt = get_grid_iter(param_grid)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for param in it:\n",
    "        \n",
    "        model = model_base(**param)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        if bin_prob:\n",
    "            y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        train_score = metric(y_train, y_train_pred)\n",
    "        val_score = metric(y_val, y_val_pred)\n",
    "        \n",
    "        if val_score > best_score:\n",
    "            best_score = val_score\n",
    "            best_param_id = step\n",
    "        \n",
    "        step += 1\n",
    "        if verbose:\n",
    "            print(\"[%d/%d] train:%f test:%f best:%f id:%d\" %\n",
    "                (step, item_cnt, train_score, val_score, best_score, best_param_id))\n",
    "            \n",
    "    return best_score, best_param, best_param_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [500, 1000, 1500, 2000, 3000],\n",
    "    'max_depth':[1, 2, 3, 4, 5],\n",
    "    'objective':'binary:logistic',\n",
    "    'subsample':[0.6, 0.8, 1], \n",
    "    'colsample_bytree':[0.6, 0.8, 1.0],\n",
    "    'learning_rate':[0.001, 0.01, 0.1],\n",
    "    'tree_method':'gpu_hist',\n",
    "    'evalmetric': 'auc'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/675] train:0.671892 test:0.670447 best:0.670447 id:0\n",
      "[2/675] train:0.716421 test:0.715462 best:0.715462 id:1\n",
      "[3/675] train:0.761949 test:0.762208 best:0.762208 id:2\n",
      "[4/675] train:0.668143 test:0.666607 best:0.762208 id:2\n",
      "[5/675] train:0.716741 test:0.715839 best:0.762208 id:2\n",
      "[6/675] train:0.762040 test:0.762301 best:0.762301 id:5\n",
      "[7/675] train:0.668453 test:0.667074 best:0.762301 id:5\n",
      "[8/675] train:0.716499 test:0.715538 best:0.762301 id:5\n",
      "[9/675] train:0.762145 test:0.762317 best:0.762317 id:8\n",
      "[10/675] train:0.674085 test:0.673059 best:0.762317 id:8\n",
      "[11/675] train:0.717710 test:0.716821 best:0.762317 id:8\n",
      "[12/675] train:0.762334 test:0.762609 best:0.762609 id:11\n",
      "[13/675] train:0.670082 test:0.668960 best:0.762609 id:11\n",
      "[14/675] train:0.717817 test:0.716933 best:0.762609 id:11\n",
      "[15/675] train:0.762335 test:0.762573 best:0.762609 id:11\n",
      "[16/675] train:0.669924 test:0.668792 best:0.762609 id:11\n",
      "[17/675] train:0.717632 test:0.716733 best:0.762609 id:11\n",
      "[18/675] train:0.762460 test:0.762715 best:0.762715 id:17\n",
      "[19/675] train:0.675564 test:0.675131 best:0.762715 id:17\n",
      "[20/675] train:0.718918 test:0.718031 best:0.762715 id:17\n",
      "[21/675] train:0.762640 test:0.762880 best:0.762880 id:20\n",
      "[22/675] train:0.670180 test:0.669661 best:0.762880 id:20\n",
      "[23/675] train:0.718742 test:0.717865 best:0.762880 id:20\n",
      "[24/675] train:0.762645 test:0.762872 best:0.762880 id:20\n",
      "[25/675] train:0.670180 test:0.669661 best:0.762880 id:20\n",
      "[26/675] train:0.718742 test:0.717865 best:0.762880 id:20\n",
      "[27/675] train:0.762641 test:0.762865 best:0.762880 id:20\n",
      "[28/675] train:0.696187 test:0.694469 best:0.762880 id:20\n",
      "[29/675] train:0.736115 test:0.734608 best:0.762880 id:20\n",
      "[30/675] train:0.771677 test:0.768999 best:0.768999 id:29\n",
      "[31/675] train:0.687201 test:0.685040 best:0.768999 id:29\n",
      "[32/675] train:0.736449 test:0.734878 best:0.768999 id:29\n",
      "[33/675] train:0.771807 test:0.768983 best:0.768999 id:29\n",
      "[34/675] train:0.684878 test:0.682626 best:0.768999 id:29\n",
      "[35/675] train:0.736777 test:0.735186 best:0.768999 id:29\n",
      "[36/675] train:0.772091 test:0.769053 best:0.769053 id:35\n",
      "[37/675] train:0.693998 test:0.692225 best:0.769053 id:35\n",
      "[38/675] train:0.735449 test:0.733762 best:0.769053 id:35\n",
      "[39/675] train:0.771490 test:0.768612 best:0.769053 id:35\n",
      "[40/675] train:0.686097 test:0.683989 best:0.769053 id:35\n",
      "[41/675] train:0.735500 test:0.733651 best:0.769053 id:35\n",
      "[42/675] train:0.771923 test:0.768956 best:0.769053 id:35\n",
      "[43/675] train:0.683246 test:0.680927 best:0.769053 id:35\n",
      "[44/675] train:0.735320 test:0.733353 best:0.769053 id:35\n",
      "[45/675] train:0.771994 test:0.769024 best:0.769053 id:35\n",
      "[46/675] train:0.692185 test:0.690440 best:0.769053 id:35\n",
      "[47/675] train:0.734874 test:0.732453 best:0.769053 id:35\n",
      "[48/675] train:0.771542 test:0.768143 best:0.769053 id:35\n",
      "[49/675] train:0.683601 test:0.681280 best:0.769053 id:35\n",
      "[50/675] train:0.734742 test:0.732225 best:0.769053 id:35\n",
      "[51/675] train:0.772430 test:0.768769 best:0.769053 id:35\n",
      "[52/675] train:0.680894 test:0.678375 best:0.769053 id:35\n",
      "[53/675] train:0.734900 test:0.732361 best:0.769053 id:35\n",
      "[54/675] train:0.772170 test:0.768598 best:0.769053 id:35\n",
      "[55/675] train:0.710368 test:0.708519 best:0.769053 id:35\n",
      "[56/675] train:0.747412 test:0.744911 best:0.769053 id:35\n",
      "[57/675] train:0.780829 test:0.772765 best:0.772765 id:56\n",
      "[58/675] train:0.698674 test:0.696645 best:0.772765 id:56\n",
      "[59/675] train:0.747878 test:0.744940 best:0.772765 id:56\n",
      "[60/675] train:0.781643 test:0.772982 best:0.772982 id:59\n",
      "[61/675] train:0.693157 test:0.690709 best:0.772982 id:59\n",
      "[62/675] train:0.748756 test:0.745696 best:0.772982 id:59\n",
      "[63/675] train:0.782018 test:0.772837 best:0.772982 id:59\n",
      "[64/675] train:0.708974 test:0.707038 best:0.772982 id:59\n",
      "[65/675] train:0.746865 test:0.743905 best:0.772982 id:59\n",
      "[66/675] train:0.781334 test:0.773086 best:0.773086 id:65\n",
      "[67/675] train:0.697133 test:0.694776 best:0.773086 id:65\n",
      "[68/675] train:0.748101 test:0.744565 best:0.773086 id:65\n",
      "[69/675] train:0.782164 test:0.773060 best:0.773086 id:65\n",
      "[70/675] train:0.691462 test:0.688953 best:0.773086 id:65\n",
      "[71/675] train:0.748561 test:0.744896 best:0.773086 id:65\n",
      "[72/675] train:0.782470 test:0.773190 best:0.773190 id:71\n",
      "[73/675] train:0.707148 test:0.705015 best:0.773190 id:71\n",
      "[74/675] train:0.747314 test:0.743640 best:0.773190 id:71\n",
      "[75/675] train:0.780882 test:0.771953 best:0.773190 id:71\n",
      "[76/675] train:0.694706 test:0.691775 best:0.773190 id:71\n",
      "[77/675] train:0.747878 test:0.743402 best:0.773190 id:71\n",
      "[78/675] train:0.781656 test:0.772144 best:0.773190 id:71\n",
      "[79/675] train:0.689469 test:0.686428 best:0.773190 id:71\n",
      "[80/675] train:0.748849 test:0.744232 best:0.773190 id:71\n",
      "[81/675] train:0.781579 test:0.771557 best:0.773190 id:71\n",
      "[82/675] train:0.722829 test:0.720564 best:0.773190 id:71\n",
      "[83/675] train:0.756066 test:0.751952 best:0.773190 id:71\n",
      "[84/675] train:0.794042 test:0.774997 best:0.774997 id:83\n",
      "[85/675] train:0.710267 test:0.707946 best:0.774997 id:83\n",
      "[86/675] train:0.757175 test:0.752390 best:0.774997 id:83\n",
      "[87/675] train:0.795195 test:0.774283 best:0.774997 id:83\n",
      "[88/675] train:0.702246 test:0.699443 best:0.774997 id:83\n",
      "[89/675] train:0.758179 test:0.753092 best:0.774997 id:83\n",
      "[90/675] train:0.795962 test:0.774993 best:0.774997 id:83\n",
      "[91/675] train:0.721695 test:0.719361 best:0.774997 id:83\n",
      "[92/675] train:0.755681 test:0.750981 best:0.774997 id:83\n",
      "[93/675] train:0.794530 test:0.775515 best:0.775515 id:92\n",
      "[94/675] train:0.708830 test:0.706345 best:0.775515 id:92\n",
      "[95/675] train:0.757260 test:0.751649 best:0.775515 id:92\n",
      "[96/675] train:0.795742 test:0.775374 best:0.775515 id:92\n",
      "[97/675] train:0.700905 test:0.697934 best:0.775515 id:92\n",
      "[98/675] train:0.758300 test:0.752213 best:0.775515 id:92\n",
      "[99/675] train:0.796706 test:0.774931 best:0.775515 id:92\n",
      "[100/675] train:0.719616 test:0.717273 best:0.775515 id:92\n",
      "[101/675] train:0.756321 test:0.750802 best:0.775515 id:92\n",
      "[102/675] train:0.794230 test:0.774404 best:0.775515 id:92\n",
      "[103/675] train:0.707156 test:0.704410 best:0.775515 id:92\n",
      "[104/675] train:0.757467 test:0.750795 best:0.775515 id:92\n",
      "[105/675] train:0.795193 test:0.774042 best:0.775515 id:92\n",
      "[106/675] train:0.698808 test:0.695649 best:0.775515 id:92\n",
      "[107/675] train:0.758759 test:0.751653 best:0.775515 id:92\n",
      "[108/675] train:0.795708 test:0.773646 best:0.775515 id:92\n",
      "[109/675] train:0.732680 test:0.729024 best:0.775515 id:92\n",
      "[110/675] train:0.763322 test:0.756213 best:0.775515 id:92\n",
      "[111/675] train:0.812980 test:0.774521 best:0.775515 id:92\n",
      "[112/675] train:0.721908 test:0.718161 best:0.775515 id:92\n",
      "[113/675] train:0.764683 test:0.756648 best:0.775515 id:92\n",
      "[114/675] train:0.815593 test:0.774227 best:0.775515 id:92\n",
      "[115/675] train:0.712013 test:0.708241 best:0.775515 id:92\n",
      "[116/675] train:0.765862 test:0.757102 best:0.775515 id:92\n",
      "[117/675] train:0.817374 test:0.774531 best:0.775515 id:92\n",
      "[118/675] train:0.731706 test:0.727995 best:0.775515 id:92\n",
      "[119/675] train:0.763252 test:0.755267 best:0.775515 id:92\n",
      "[120/675] train:0.814139 test:0.775431 best:0.775515 id:92\n",
      "[121/675] train:0.720824 test:0.717065 best:0.775515 id:92\n",
      "[122/675] train:0.765064 test:0.756048 best:0.775515 id:92\n",
      "[123/675] train:0.817333 test:0.775355 best:0.775515 id:92\n",
      "[124/675] train:0.710717 test:0.706803 best:0.775515 id:92\n",
      "[125/675] train:0.766142 test:0.756241 best:0.775515 id:92\n",
      "[126/675] train:0.819007 test:0.775093 best:0.775515 id:92\n",
      "[127/675] train:0.729918 test:0.726224 best:0.775515 id:92\n",
      "[128/675] train:0.763681 test:0.754802 best:0.775515 id:92\n",
      "[129/675] train:0.814461 test:0.775329 best:0.775515 id:92\n",
      "[130/675] train:0.719323 test:0.715473 best:0.775515 id:92\n",
      "[131/675] train:0.765349 test:0.755040 best:0.775515 id:92\n",
      "[132/675] train:0.816594 test:0.774818 best:0.775515 id:92\n",
      "[133/675] train:0.708953 test:0.704864 best:0.775515 id:92\n",
      "[134/675] train:0.766590 test:0.755503 best:0.775515 id:92\n",
      "[135/675] train:0.817517 test:0.774082 best:0.775515 id:92\n",
      "[136/675] train:0.684877 test:0.683503 best:0.775515 id:92\n",
      "[137/675] train:0.731769 test:0.731141 best:0.775515 id:92\n",
      "[138/675] train:0.766776 test:0.766833 best:0.775515 id:92\n",
      "[139/675] train:0.681757 test:0.680248 best:0.775515 id:92\n",
      "[140/675] train:0.731920 test:0.731289 best:0.775515 id:92\n",
      "[141/675] train:0.766883 test:0.766898 best:0.775515 id:92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142/675] train:0.680569 test:0.679047 best:0.775515 id:92\n",
      "[143/675] train:0.732031 test:0.731360 best:0.775515 id:92\n",
      "[144/675] train:0.766940 test:0.766940 best:0.775515 id:92\n",
      "[145/675] train:0.686542 test:0.685249 best:0.775515 id:92\n",
      "[146/675] train:0.732895 test:0.732318 best:0.775515 id:92\n",
      "[147/675] train:0.766802 test:0.766877 best:0.775515 id:92\n",
      "[148/675] train:0.682701 test:0.681313 best:0.775515 id:92\n",
      "[149/675] train:0.732969 test:0.732375 best:0.775515 id:92\n",
      "[150/675] train:0.766829 test:0.766867 best:0.775515 id:92\n",
      "[151/675] train:0.682652 test:0.681199 best:0.775515 id:92\n",
      "[152/675] train:0.732903 test:0.732282 best:0.775515 id:92\n",
      "[153/675] train:0.766885 test:0.766942 best:0.775515 id:92\n",
      "[154/675] train:0.686341 test:0.685503 best:0.775515 id:92\n",
      "[155/675] train:0.734013 test:0.733442 best:0.775515 id:92\n",
      "[156/675] train:0.766669 test:0.766727 best:0.775515 id:92\n",
      "[157/675] train:0.681016 test:0.679888 best:0.775515 id:92\n",
      "[158/675] train:0.734005 test:0.733428 best:0.775515 id:92\n",
      "[159/675] train:0.766672 test:0.766735 best:0.775515 id:92\n",
      "[160/675] train:0.681016 test:0.679888 best:0.775515 id:92\n",
      "[161/675] train:0.734005 test:0.733428 best:0.775515 id:92\n",
      "[162/675] train:0.766674 test:0.766737 best:0.775515 id:92\n",
      "[163/675] train:0.701096 test:0.699460 best:0.775515 id:92\n",
      "[164/675] train:0.752353 test:0.750880 best:0.775515 id:92\n",
      "[165/675] train:0.779343 test:0.774680 best:0.775515 id:92\n",
      "[166/675] train:0.698085 test:0.696237 best:0.775515 id:92\n",
      "[167/675] train:0.752628 test:0.751053 best:0.775515 id:92\n",
      "[168/675] train:0.779275 test:0.774461 best:0.775515 id:92\n",
      "[169/675] train:0.697782 test:0.695799 best:0.775515 id:92\n",
      "[170/675] train:0.753081 test:0.751492 best:0.775515 id:92\n",
      "[171/675] train:0.779923 test:0.774802 best:0.775515 id:92\n",
      "[172/675] train:0.699605 test:0.697813 best:0.775515 id:92\n",
      "[173/675] train:0.752651 test:0.750925 best:0.775515 id:92\n",
      "[174/675] train:0.779079 test:0.774347 best:0.775515 id:92\n",
      "[175/675] train:0.696543 test:0.694633 best:0.775515 id:92\n",
      "[176/675] train:0.752866 test:0.750955 best:0.775515 id:92\n",
      "[177/675] train:0.779303 test:0.774358 best:0.775515 id:92\n",
      "[178/675] train:0.696489 test:0.694470 best:0.775515 id:92\n",
      "[179/675] train:0.753250 test:0.751267 best:0.775515 id:92\n",
      "[180/675] train:0.779370 test:0.774443 best:0.775515 id:92\n",
      "[181/675] train:0.698416 test:0.696279 best:0.775515 id:92\n",
      "[182/675] train:0.753066 test:0.750506 best:0.775515 id:92\n",
      "[183/675] train:0.778413 test:0.773358 best:0.775515 id:92\n",
      "[184/675] train:0.695745 test:0.693402 best:0.775515 id:92\n",
      "[185/675] train:0.753254 test:0.750620 best:0.775515 id:92\n",
      "[186/675] train:0.778609 test:0.773328 best:0.775515 id:92\n",
      "[187/675] train:0.695703 test:0.693336 best:0.775515 id:92\n",
      "[188/675] train:0.753465 test:0.750857 best:0.775515 id:92\n",
      "[189/675] train:0.778393 test:0.773142 best:0.775515 id:92\n",
      "[190/675] train:0.714350 test:0.712429 best:0.775515 id:92\n",
      "[191/675] train:0.761648 test:0.758589 best:0.775515 id:92\n",
      "[192/675] train:0.791846 test:0.777599 best:0.777599 id:191\n",
      "[193/675] train:0.706936 test:0.704629 best:0.777599 id:191\n",
      "[194/675] train:0.762556 test:0.759162 best:0.777599 id:191\n",
      "[195/675] train:0.792946 test:0.777793 best:0.777793 id:194\n",
      "[196/675] train:0.705552 test:0.703055 best:0.777793 id:194\n",
      "[197/675] train:0.763170 test:0.759650 best:0.777793 id:194\n",
      "[198/675] train:0.793332 test:0.777380 best:0.777793 id:194\n",
      "[199/675] train:0.713036 test:0.710972 best:0.777793 id:194\n",
      "[200/675] train:0.761915 test:0.758158 best:0.777793 id:194\n",
      "[201/675] train:0.792262 test:0.777902 best:0.777902 id:200\n",
      "[202/675] train:0.705784 test:0.703287 best:0.777902 id:200\n",
      "[203/675] train:0.762908 test:0.758758 best:0.777902 id:200\n",
      "[204/675] train:0.793195 test:0.777822 best:0.777902 id:200\n",
      "[205/675] train:0.704360 test:0.701619 best:0.777902 id:200\n",
      "[206/675] train:0.763564 test:0.759389 best:0.777902 id:200\n",
      "[207/675] train:0.793665 test:0.777935 best:0.777935 id:206\n",
      "[208/675] train:0.712247 test:0.709985 best:0.777935 id:206\n",
      "[209/675] train:0.762644 test:0.758086 best:0.777935 id:206\n",
      "[210/675] train:0.791518 test:0.777114 best:0.777935 id:206\n",
      "[211/675] train:0.704325 test:0.701513 best:0.777935 id:206\n",
      "[212/675] train:0.763400 test:0.758368 best:0.777935 id:206\n",
      "[213/675] train:0.792136 test:0.777024 best:0.777935 id:206\n",
      "[214/675] train:0.703872 test:0.700824 best:0.777935 id:206\n",
      "[215/675] train:0.764190 test:0.759063 best:0.777935 id:206\n",
      "[216/675] train:0.792148 test:0.776626 best:0.777935 id:206\n",
      "[217/675] train:0.726508 test:0.724079 best:0.777935 id:206\n",
      "[218/675] train:0.767849 test:0.762203 best:0.777935 id:206\n",
      "[219/675] train:0.810569 test:0.777257 best:0.777935 id:206\n",
      "[220/675] train:0.717986 test:0.715279 best:0.777935 id:206\n",
      "[221/675] train:0.768816 test:0.762530 best:0.777935 id:206\n",
      "[222/675] train:0.812465 test:0.776501 best:0.777935 id:206\n",
      "[223/675] train:0.715119 test:0.711930 best:0.777935 id:206\n",
      "[224/675] train:0.769511 test:0.762900 best:0.777935 id:206\n",
      "[225/675] train:0.813560 test:0.777187 best:0.777935 id:206\n",
      "[226/675] train:0.725595 test:0.723008 best:0.777935 id:206\n",
      "[227/675] train:0.768057 test:0.761640 best:0.777935 id:206\n",
      "[228/675] train:0.811398 test:0.778324 best:0.778324 id:227\n",
      "[229/675] train:0.716724 test:0.713800 best:0.778324 id:227\n",
      "[230/675] train:0.769174 test:0.761946 best:0.778324 id:227\n",
      "[231/675] train:0.813498 test:0.778349 best:0.778349 id:230\n",
      "[232/675] train:0.714080 test:0.710552 best:0.778349 id:230\n",
      "[233/675] train:0.770005 test:0.762428 best:0.778349 id:230\n",
      "[234/675] train:0.814856 test:0.777510 best:0.778349 id:230\n",
      "[235/675] train:0.724350 test:0.721497 best:0.778349 id:230\n",
      "[236/675] train:0.768757 test:0.761253 best:0.778349 id:230\n",
      "[237/675] train:0.810923 test:0.777532 best:0.778349 id:230\n",
      "[238/675] train:0.715497 test:0.712152 best:0.778349 id:230\n",
      "[239/675] train:0.769801 test:0.761456 best:0.778349 id:230\n",
      "[240/675] train:0.812547 test:0.777349 best:0.778349 id:230\n",
      "[241/675] train:0.713174 test:0.709008 best:0.778349 id:230\n",
      "[242/675] train:0.770595 test:0.761844 best:0.778349 id:230\n",
      "[243/675] train:0.813149 test:0.776639 best:0.778349 id:230\n",
      "[244/675] train:0.736035 test:0.732018 best:0.778349 id:230\n",
      "[245/675] train:0.775192 test:0.764457 best:0.778349 id:230\n",
      "[246/675] train:0.840733 test:0.774480 best:0.778349 id:230\n",
      "[247/675] train:0.727616 test:0.723475 best:0.778349 id:230\n",
      "[248/675] train:0.776501 test:0.764764 best:0.778349 id:230\n",
      "[249/675] train:0.844404 test:0.773779 best:0.778349 id:230\n",
      "[250/675] train:0.723618 test:0.719071 best:0.778349 id:230\n",
      "[251/675] train:0.777564 test:0.764950 best:0.778349 id:230\n",
      "[252/675] train:0.846936 test:0.773631 best:0.778349 id:230\n",
      "[253/675] train:0.735180 test:0.730956 best:0.778349 id:230\n",
      "[254/675] train:0.775475 test:0.763757 best:0.778349 id:230\n",
      "[255/675] train:0.842993 test:0.776587 best:0.778349 id:230\n",
      "[256/675] train:0.726552 test:0.722214 best:0.778349 id:230\n",
      "[257/675] train:0.776953 test:0.764203 best:0.778349 id:230\n",
      "[258/675] train:0.847076 test:0.775438 best:0.778349 id:230\n",
      "[259/675] train:0.722564 test:0.717671 best:0.778349 id:230\n",
      "[260/675] train:0.778043 test:0.764448 best:0.778349 id:230\n",
      "[261/675] train:0.849371 test:0.774943 best:0.778349 id:230\n",
      "[262/675] train:0.734214 test:0.729777 best:0.778349 id:230\n",
      "[263/675] train:0.776214 test:0.763382 best:0.778349 id:230\n",
      "[264/675] train:0.842465 test:0.775936 best:0.778349 id:230\n",
      "[265/675] train:0.725742 test:0.721045 best:0.778349 id:230\n",
      "[266/675] train:0.777548 test:0.763313 best:0.778349 id:230\n",
      "[267/675] train:0.845230 test:0.775284 best:0.778349 id:230\n",
      "[268/675] train:0.721731 test:0.716301 best:0.778349 id:230\n",
      "[269/675] train:0.778452 test:0.763532 best:0.778349 id:230\n",
      "[270/675] train:0.847832 test:0.774634 best:0.778349 id:230\n",
      "[271/675] train:0.689305 test:0.687908 best:0.778349 id:230\n",
      "[272/675] train:0.741202 test:0.740848 best:0.778349 id:230\n",
      "[273/675] train:0.768821 test:0.768724 best:0.778349 id:230\n",
      "[274/675] train:0.686793 test:0.685255 best:0.778349 id:230\n",
      "[275/675] train:0.741335 test:0.741011 best:0.778349 id:230\n",
      "[276/675] train:0.768935 test:0.768841 best:0.778349 id:230\n",
      "[277/675] train:0.687023 test:0.685535 best:0.778349 id:230\n",
      "[278/675] train:0.741279 test:0.740929 best:0.778349 id:230\n",
      "[279/675] train:0.769000 test:0.768865 best:0.778349 id:230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[280/675] train:0.690390 test:0.689021 best:0.778349 id:230\n",
      "[281/675] train:0.742455 test:0.742115 best:0.778349 id:230\n",
      "[282/675] train:0.768664 test:0.768641 best:0.778349 id:230\n",
      "[283/675] train:0.688587 test:0.687224 best:0.778349 id:230\n",
      "[284/675] train:0.742591 test:0.742262 best:0.778349 id:230\n",
      "[285/675] train:0.768727 test:0.768676 best:0.778349 id:230\n",
      "[286/675] train:0.688879 test:0.687517 best:0.778349 id:230\n",
      "[287/675] train:0.742586 test:0.742236 best:0.778349 id:230\n",
      "[288/675] train:0.768785 test:0.768752 best:0.778349 id:230\n",
      "[289/675] train:0.690412 test:0.689547 best:0.778349 id:230\n",
      "[290/675] train:0.743840 test:0.743558 best:0.778349 id:230\n",
      "[291/675] train:0.768338 test:0.768317 best:0.778349 id:230\n",
      "[292/675] train:0.688804 test:0.687918 best:0.778349 id:230\n",
      "[293/675] train:0.743844 test:0.743563 best:0.778349 id:230\n",
      "[294/675] train:0.768335 test:0.768305 best:0.778349 id:230\n",
      "[295/675] train:0.688804 test:0.687918 best:0.778349 id:230\n",
      "[296/675] train:0.743844 test:0.743563 best:0.778349 id:230\n",
      "[297/675] train:0.768330 test:0.768291 best:0.778349 id:230\n",
      "[298/675] train:0.707587 test:0.705980 best:0.778349 id:230\n",
      "[299/675] train:0.759490 test:0.757986 best:0.778349 id:230\n",
      "[300/675] train:0.783433 test:0.777096 best:0.778349 id:230\n",
      "[301/675] train:0.705851 test:0.704072 best:0.778349 id:230\n",
      "[302/675] train:0.759721 test:0.758057 best:0.778349 id:230\n",
      "[303/675] train:0.783682 test:0.777198 best:0.778349 id:230\n",
      "[304/675] train:0.705900 test:0.704022 best:0.778349 id:230\n",
      "[305/675] train:0.759961 test:0.758317 best:0.778349 id:230\n",
      "[306/675] train:0.784093 test:0.777188 best:0.778349 id:230\n",
      "[307/675] train:0.706003 test:0.704160 best:0.778349 id:230\n",
      "[308/675] train:0.759734 test:0.757908 best:0.778349 id:230\n",
      "[309/675] train:0.783420 test:0.776962 best:0.778349 id:230\n",
      "[310/675] train:0.704308 test:0.702353 best:0.778349 id:230\n",
      "[311/675] train:0.759916 test:0.757986 best:0.778349 id:230\n",
      "[312/675] train:0.783901 test:0.777263 best:0.778349 id:230\n",
      "[313/675] train:0.704425 test:0.702439 best:0.778349 id:230\n",
      "[314/675] train:0.760023 test:0.758006 best:0.778349 id:230\n",
      "[315/675] train:0.783997 test:0.777377 best:0.778349 id:230\n",
      "[316/675] train:0.704986 test:0.702898 best:0.778349 id:230\n",
      "[317/675] train:0.760242 test:0.757485 best:0.778349 id:230\n",
      "[318/675] train:0.782854 test:0.776382 best:0.778349 id:230\n",
      "[319/675] train:0.703550 test:0.701335 best:0.778349 id:230\n",
      "[320/675] train:0.760531 test:0.757802 best:0.778349 id:230\n",
      "[321/675] train:0.783199 test:0.776504 best:0.778349 id:230\n",
      "[322/675] train:0.703647 test:0.701362 best:0.778349 id:230\n",
      "[323/675] train:0.760633 test:0.757941 best:0.778349 id:230\n",
      "[324/675] train:0.782988 test:0.776243 best:0.778349 id:230\n",
      "[325/675] train:0.719646 test:0.717651 best:0.778349 id:230\n",
      "[326/675] train:0.766796 test:0.763268 best:0.778349 id:230\n",
      "[327/675] train:0.798395 test:0.778897 best:0.778897 id:326\n",
      "[328/675] train:0.716951 test:0.714445 best:0.778897 id:326\n",
      "[329/675] train:0.767375 test:0.763528 best:0.778897 id:326\n",
      "[330/675] train:0.799465 test:0.778980 best:0.778980 id:329\n",
      "[331/675] train:0.717144 test:0.714441 best:0.778980 id:329\n",
      "[332/675] train:0.767765 test:0.763779 best:0.778980 id:329\n",
      "[333/675] train:0.800288 test:0.778503 best:0.778980 id:329\n",
      "[334/675] train:0.718497 test:0.716327 best:0.778980 id:329\n",
      "[335/675] train:0.766965 test:0.762696 best:0.778980 id:329\n",
      "[336/675] train:0.799204 test:0.779536 best:0.779536 id:335\n",
      "[337/675] train:0.716184 test:0.713431 best:0.779536 id:335\n",
      "[338/675] train:0.767645 test:0.763069 best:0.779536 id:335\n",
      "[339/675] train:0.800198 test:0.779268 best:0.779536 id:335\n",
      "[340/675] train:0.716397 test:0.713337 best:0.779536 id:335\n",
      "[341/675] train:0.768151 test:0.763513 best:0.779536 id:335\n",
      "[342/675] train:0.800916 test:0.779510 best:0.779536 id:335\n",
      "[343/675] train:0.717576 test:0.715177 best:0.779536 id:335\n",
      "[344/675] train:0.767606 test:0.762606 best:0.779536 id:335\n",
      "[345/675] train:0.798257 test:0.779253 best:0.779536 id:335\n",
      "[346/675] train:0.715627 test:0.712342 best:0.779536 id:335\n",
      "[347/675] train:0.768227 test:0.762758 best:0.779536 id:335\n",
      "[348/675] train:0.798928 test:0.778880 best:0.779536 id:335\n",
      "[349/675] train:0.715798 test:0.712276 best:0.779536 id:335\n",
      "[350/675] train:0.768673 test:0.763118 best:0.779536 id:335\n",
      "[351/675] train:0.799168 test:0.778770 best:0.779536 id:335\n",
      "[352/675] train:0.729965 test:0.727334 best:0.779536 id:335\n",
      "[353/675] train:0.773381 test:0.766087 best:0.779536 id:335\n",
      "[354/675] train:0.822501 test:0.777024 best:0.779536 id:335\n",
      "[355/675] train:0.725426 test:0.722361 best:0.779536 id:335\n",
      "[356/675] train:0.774320 test:0.766346 best:0.779536 id:335\n",
      "[357/675] train:0.824655 test:0.776201 best:0.779536 id:335\n",
      "[358/675] train:0.725290 test:0.721657 best:0.779536 id:335\n",
      "[359/675] train:0.774911 test:0.766497 best:0.779536 id:335\n",
      "[360/675] train:0.826195 test:0.777113 best:0.779536 id:335\n",
      "[361/675] train:0.729086 test:0.726283 best:0.779536 id:335\n",
      "[362/675] train:0.773527 test:0.765592 best:0.779536 id:335\n",
      "[363/675] train:0.823852 test:0.778306 best:0.779536 id:335\n",
      "[364/675] train:0.724744 test:0.721356 best:0.779536 id:335\n",
      "[365/675] train:0.774448 test:0.765637 best:0.779536 id:335\n",
      "[366/675] train:0.826243 test:0.778171 best:0.779536 id:335\n",
      "[367/675] train:0.724712 test:0.720659 best:0.779536 id:335\n",
      "[368/675] train:0.775157 test:0.765926 best:0.779536 id:335\n",
      "[369/675] train:0.827993 test:0.777405 best:0.779536 id:335\n",
      "[370/675] train:0.728392 test:0.725278 best:0.779536 id:335\n",
      "[371/675] train:0.774055 test:0.764978 best:0.779536 id:335\n",
      "[372/675] train:0.823436 test:0.778012 best:0.779536 id:335\n",
      "[373/675] train:0.724149 test:0.720028 best:0.779536 id:335\n",
      "[374/675] train:0.774936 test:0.765043 best:0.779536 id:335\n",
      "[375/675] train:0.824892 test:0.777654 best:0.779536 id:335\n",
      "[376/675] train:0.724009 test:0.719296 best:0.779536 id:335\n",
      "[377/675] train:0.775356 test:0.765011 best:0.779536 id:335\n",
      "[378/675] train:0.826244 test:0.776816 best:0.779536 id:335\n",
      "[379/675] train:0.739362 test:0.734979 best:0.779536 id:335\n",
      "[380/675] train:0.782731 test:0.768289 best:0.779536 id:335\n",
      "[381/675] train:0.861652 test:0.772451 best:0.779536 id:335\n",
      "[382/675] train:0.733454 test:0.728828 best:0.779536 id:335\n",
      "[383/675] train:0.784394 test:0.768675 best:0.779536 id:335\n",
      "[384/675] train:0.866350 test:0.771801 best:0.779536 id:335\n",
      "[385/675] train:0.732200 test:0.726979 best:0.779536 id:335\n",
      "[386/675] train:0.785578 test:0.768773 best:0.779536 id:335\n",
      "[387/675] train:0.869212 test:0.771121 best:0.779536 id:335\n",
      "[388/675] train:0.738671 test:0.734010 best:0.779536 id:335\n",
      "[389/675] train:0.783015 test:0.767635 best:0.779536 id:335\n",
      "[390/675] train:0.864682 test:0.774897 best:0.779536 id:335\n",
      "[391/675] train:0.732573 test:0.727639 best:0.779536 id:335\n",
      "[392/675] train:0.784644 test:0.768041 best:0.779536 id:335\n",
      "[393/675] train:0.869350 test:0.773813 best:0.779536 id:335\n",
      "[394/675] train:0.731642 test:0.725990 best:0.779536 id:335\n",
      "[395/675] train:0.785984 test:0.768376 best:0.779536 id:335\n",
      "[396/675] train:0.872587 test:0.772978 best:0.779536 id:335\n",
      "[397/675] train:0.738076 test:0.733094 best:0.779536 id:335\n",
      "[398/675] train:0.783552 test:0.767103 best:0.779536 id:335\n",
      "[399/675] train:0.864066 test:0.774449 best:0.779536 id:335\n",
      "[400/675] train:0.731830 test:0.726324 best:0.779536 id:335\n",
      "[401/675] train:0.784783 test:0.766812 best:0.779536 id:335\n",
      "[402/675] train:0.867924 test:0.774150 best:0.779536 id:335\n",
      "[403/675] train:0.731206 test:0.724663 best:0.779536 id:335\n",
      "[404/675] train:0.785479 test:0.766705 best:0.779536 id:335\n",
      "[405/675] train:0.870614 test:0.772842 best:0.779536 id:335\n",
      "[406/675] train:0.694653 test:0.693402 best:0.779536 id:335\n",
      "[407/675] train:0.748087 test:0.747928 best:0.779536 id:335\n",
      "[408/675] train:0.770092 test:0.769809 best:0.779536 id:335\n",
      "[409/675] train:0.693855 test:0.692636 best:0.779536 id:335\n",
      "[410/675] train:0.748148 test:0.748023 best:0.779536 id:335\n",
      "[411/675] train:0.770210 test:0.769926 best:0.779536 id:335\n",
      "[412/675] train:0.694192 test:0.692967 best:0.779536 id:335\n",
      "[413/675] train:0.748285 test:0.748164 best:0.779536 id:335\n",
      "[414/675] train:0.770279 test:0.769919 best:0.779536 id:335\n",
      "[415/675] train:0.695903 test:0.694733 best:0.779536 id:335\n",
      "[416/675] train:0.749138 test:0.749026 best:0.779536 id:335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[417/675] train:0.769859 test:0.769700 best:0.779536 id:335\n",
      "[418/675] train:0.694992 test:0.693834 best:0.779536 id:335\n",
      "[419/675] train:0.749116 test:0.749025 best:0.779536 id:335\n",
      "[420/675] train:0.769905 test:0.769716 best:0.779536 id:335\n",
      "[421/675] train:0.694983 test:0.693810 best:0.779536 id:335\n",
      "[422/675] train:0.749233 test:0.749133 best:0.779536 id:335\n",
      "[423/675] train:0.769991 test:0.769823 best:0.779536 id:335\n",
      "[424/675] train:0.696226 test:0.695085 best:0.779536 id:335\n",
      "[425/675] train:0.750101 test:0.750036 best:0.779536 id:335\n",
      "[426/675] train:0.769349 test:0.769263 best:0.779536 id:335\n",
      "[427/675] train:0.695319 test:0.694174 best:0.779536 id:335\n",
      "[428/675] train:0.750118 test:0.750050 best:0.779536 id:335\n",
      "[429/675] train:0.769345 test:0.769262 best:0.779536 id:335\n",
      "[430/675] train:0.695320 test:0.694175 best:0.779536 id:335\n",
      "[431/675] train:0.750119 test:0.750060 best:0.779536 id:335\n",
      "[432/675] train:0.769345 test:0.769256 best:0.779536 id:335\n",
      "[433/675] train:0.714181 test:0.712557 best:0.779536 id:335\n",
      "[434/675] train:0.762882 test:0.761297 best:0.779536 id:335\n",
      "[435/675] train:0.786707 test:0.778821 best:0.779536 id:335\n",
      "[436/675] train:0.714010 test:0.712237 best:0.779536 id:335\n",
      "[437/675] train:0.763283 test:0.761540 best:0.779536 id:335\n",
      "[438/675] train:0.786938 test:0.778798 best:0.779536 id:335\n",
      "[439/675] train:0.714121 test:0.712269 best:0.779536 id:335\n",
      "[440/675] train:0.763531 test:0.761812 best:0.779536 id:335\n",
      "[441/675] train:0.787304 test:0.778633 best:0.779536 id:335\n",
      "[442/675] train:0.712970 test:0.711114 best:0.779536 id:335\n",
      "[443/675] train:0.763205 test:0.761251 best:0.779536 id:335\n",
      "[444/675] train:0.786818 test:0.778952 best:0.779536 id:335\n",
      "[445/675] train:0.712892 test:0.710923 best:0.779536 id:335\n",
      "[446/675] train:0.763527 test:0.761489 best:0.779536 id:335\n",
      "[447/675] train:0.787222 test:0.779006 best:0.779536 id:335\n",
      "[448/675] train:0.712967 test:0.710934 best:0.779536 id:335\n",
      "[449/675] train:0.763695 test:0.761582 best:0.779536 id:335\n",
      "[450/675] train:0.787227 test:0.779125 best:0.779536 id:335\n",
      "[451/675] train:0.712151 test:0.709971 best:0.779536 id:335\n",
      "[452/675] train:0.763958 test:0.761095 best:0.779536 id:335\n",
      "[453/675] train:0.786058 test:0.778422 best:0.779536 id:335\n",
      "[454/675] train:0.712249 test:0.709878 best:0.779536 id:335\n",
      "[455/675] train:0.764168 test:0.761288 best:0.779536 id:335\n",
      "[456/675] train:0.785951 test:0.778027 best:0.779536 id:335\n",
      "[457/675] train:0.712328 test:0.709882 best:0.779536 id:335\n",
      "[458/675] train:0.764287 test:0.761521 best:0.779536 id:335\n",
      "[459/675] train:0.786166 test:0.778185 best:0.779536 id:335\n",
      "[460/675] train:0.724729 test:0.722644 best:0.779536 id:335\n",
      "[461/675] train:0.769942 test:0.765778 best:0.779536 id:335\n",
      "[462/675] train:0.803647 test:0.779241 best:0.779536 id:335\n",
      "[463/675] train:0.723997 test:0.721407 best:0.779536 id:335\n",
      "[464/675] train:0.770470 test:0.765967 best:0.779536 id:335\n",
      "[465/675] train:0.804710 test:0.779103 best:0.779536 id:335\n",
      "[466/675] train:0.724200 test:0.721378 best:0.779536 id:335\n",
      "[467/675] train:0.770850 test:0.766179 best:0.779536 id:335\n",
      "[468/675] train:0.805561 test:0.778577 best:0.779536 id:335\n",
      "[469/675] train:0.724102 test:0.721799 best:0.779536 id:335\n",
      "[470/675] train:0.769993 test:0.765149 best:0.779536 id:335\n",
      "[471/675] train:0.804451 test:0.780097 best:0.780097 id:470\n",
      "[472/675] train:0.723421 test:0.720533 best:0.780097 id:470\n",
      "[473/675] train:0.770590 test:0.765433 best:0.780097 id:470\n",
      "[474/675] train:0.805668 test:0.779973 best:0.780097 id:470\n",
      "[475/675] train:0.723642 test:0.720455 best:0.780097 id:470\n",
      "[476/675] train:0.770925 test:0.765697 best:0.780097 id:470\n",
      "[477/675] train:0.806429 test:0.780192 best:0.780192 id:476\n",
      "[478/675] train:0.723622 test:0.720882 best:0.780192 id:476\n",
      "[479/675] train:0.770616 test:0.765019 best:0.780192 id:476\n",
      "[480/675] train:0.803274 test:0.779892 best:0.780192 id:476\n",
      "[481/675] train:0.723173 test:0.719623 best:0.780192 id:476\n",
      "[482/675] train:0.771115 test:0.765029 best:0.780192 id:476\n",
      "[483/675] train:0.804102 test:0.779665 best:0.780192 id:476\n",
      "[484/675] train:0.723339 test:0.719542 best:0.780192 id:476\n",
      "[485/675] train:0.771425 test:0.765260 best:0.780192 id:476\n",
      "[486/675] train:0.804493 test:0.779497 best:0.780192 id:476\n",
      "[487/675] train:0.733782 test:0.730940 best:0.780192 id:476\n",
      "[488/675] train:0.777578 test:0.768605 best:0.780192 id:476\n",
      "[489/675] train:0.832675 test:0.776130 best:0.780192 id:476\n",
      "[490/675] train:0.731678 test:0.728290 best:0.780192 id:476\n",
      "[491/675] train:0.778712 test:0.768911 best:0.780192 id:476\n",
      "[492/675] train:0.834997 test:0.774987 best:0.780192 id:476\n",
      "[493/675] train:0.732129 test:0.728200 best:0.780192 id:476\n",
      "[494/675] train:0.779345 test:0.769004 best:0.780192 id:476\n",
      "[495/675] train:0.837034 test:0.775858 best:0.780192 id:476\n",
      "[496/675] train:0.732987 test:0.729934 best:0.780192 id:476\n",
      "[497/675] train:0.777654 test:0.768040 best:0.780192 id:476\n",
      "[498/675] train:0.834176 test:0.777612 best:0.780192 id:476\n",
      "[499/675] train:0.731183 test:0.727406 best:0.780192 id:476\n",
      "[500/675] train:0.778753 test:0.768240 best:0.780192 id:476\n",
      "[501/675] train:0.836886 test:0.777460 best:0.780192 id:476\n",
      "[502/675] train:0.731763 test:0.727394 best:0.780192 id:476\n",
      "[503/675] train:0.779449 test:0.768394 best:0.780192 id:476\n",
      "[504/675] train:0.838996 test:0.776527 best:0.780192 id:476\n",
      "[505/675] train:0.732291 test:0.728789 best:0.780192 id:476\n",
      "[506/675] train:0.778222 test:0.767469 best:0.780192 id:476\n",
      "[507/675] train:0.833696 test:0.777720 best:0.780192 id:476\n",
      "[508/675] train:0.731126 test:0.726564 best:0.780192 id:476\n",
      "[509/675] train:0.779109 test:0.767519 best:0.780192 id:476\n",
      "[510/675] train:0.835476 test:0.777180 best:0.780192 id:476\n",
      "[511/675] train:0.731470 test:0.726350 best:0.780192 id:476\n",
      "[512/675] train:0.779472 test:0.767389 best:0.780192 id:476\n",
      "[513/675] train:0.837299 test:0.776583 best:0.780192 id:476\n",
      "[514/675] train:0.743005 test:0.738259 best:0.780192 id:476\n",
      "[515/675] train:0.789176 test:0.771073 best:0.780192 id:476\n",
      "[516/675] train:0.879284 test:0.769858 best:0.780192 id:476\n",
      "[517/675] train:0.739606 test:0.734399 best:0.780192 id:476\n",
      "[518/675] train:0.790931 test:0.771223 best:0.780192 id:476\n",
      "[519/675] train:0.884070 test:0.769369 best:0.780192 id:476\n",
      "[520/675] train:0.740159 test:0.734274 best:0.780192 id:476\n",
      "[521/675] train:0.792438 test:0.771424 best:0.780192 id:476\n",
      "[522/675] train:0.887821 test:0.768260 best:0.780192 id:476\n",
      "[523/675] train:0.742326 test:0.737222 best:0.780192 id:476\n",
      "[524/675] train:0.789533 test:0.770550 best:0.780192 id:476\n",
      "[525/675] train:0.882620 test:0.772492 best:0.780192 id:476\n",
      "[526/675] train:0.739075 test:0.733459 best:0.780192 id:476\n",
      "[527/675] train:0.791366 test:0.770822 best:0.780192 id:476\n",
      "[528/675] train:0.887584 test:0.771635 best:0.780192 id:476\n",
      "[529/675] train:0.739839 test:0.733423 best:0.780192 id:476\n",
      "[530/675] train:0.792729 test:0.770940 best:0.780192 id:476\n",
      "[531/675] train:0.890884 test:0.771123 best:0.780192 id:476\n",
      "[532/675] train:0.741919 test:0.736384 best:0.780192 id:476\n",
      "[533/675] train:0.789634 test:0.769502 best:0.780192 id:476\n",
      "[534/675] train:0.881760 test:0.772748 best:0.780192 id:476\n",
      "[535/675] train:0.738741 test:0.732422 best:0.780192 id:476\n",
      "[536/675] train:0.791072 test:0.769424 best:0.780192 id:476\n",
      "[537/675] train:0.886465 test:0.772529 best:0.780192 id:476\n",
      "[538/675] train:0.739760 test:0.732391 best:0.780192 id:476\n",
      "[539/675] train:0.791697 test:0.769142 best:0.780192 id:476\n",
      "[540/675] train:0.889271 test:0.770699 best:0.780192 id:476\n",
      "[541/675] train:0.702654 test:0.701665 best:0.780192 id:476\n",
      "[542/675] train:0.755710 test:0.755780 best:0.780192 id:476\n",
      "[543/675] train:0.771759 test:0.771119 best:0.780192 id:476\n",
      "[544/675] train:0.702227 test:0.701245 best:0.780192 id:476\n",
      "[545/675] train:0.755788 test:0.755870 best:0.780192 id:476\n",
      "[546/675] train:0.771904 test:0.771269 best:0.780192 id:476\n",
      "[547/675] train:0.702226 test:0.701239 best:0.780192 id:476\n",
      "[548/675] train:0.755879 test:0.755969 best:0.780192 id:476\n",
      "[549/675] train:0.772006 test:0.771339 best:0.780192 id:476\n",
      "[550/675] train:0.702874 test:0.701862 best:0.780192 id:476\n",
      "[551/675] train:0.756364 test:0.756463 best:0.780192 id:476\n",
      "[552/675] train:0.771443 test:0.770994 best:0.780192 id:476\n",
      "[553/675] train:0.702767 test:0.701754 best:0.780192 id:476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[554/675] train:0.756393 test:0.756507 best:0.780192 id:476\n",
      "[555/675] train:0.771558 test:0.771089 best:0.780192 id:476\n",
      "[556/675] train:0.702814 test:0.701803 best:0.780192 id:476\n",
      "[557/675] train:0.756450 test:0.756530 best:0.780192 id:476\n",
      "[558/675] train:0.771633 test:0.771166 best:0.780192 id:476\n",
      "[559/675] train:0.703809 test:0.702785 best:0.780192 id:476\n",
      "[560/675] train:0.756828 test:0.756894 best:0.780192 id:476\n",
      "[561/675] train:0.770694 test:0.770438 best:0.780192 id:476\n",
      "[562/675] train:0.703757 test:0.702732 best:0.780192 id:476\n",
      "[563/675] train:0.756811 test:0.756879 best:0.780192 id:476\n",
      "[564/675] train:0.770694 test:0.770438 best:0.780192 id:476\n",
      "[565/675] train:0.703758 test:0.702731 best:0.780192 id:476\n",
      "[566/675] train:0.756814 test:0.756883 best:0.780192 id:476\n",
      "[567/675] train:0.770693 test:0.770441 best:0.780192 id:476\n",
      "[568/675] train:0.724437 test:0.722864 best:0.780192 id:476\n",
      "[569/675] train:0.766856 test:0.764938 best:0.780192 id:476\n",
      "[570/675] train:0.791079 test:0.780488 best:0.780488 id:569\n",
      "[571/675] train:0.724366 test:0.722668 best:0.780488 id:569\n",
      "[572/675] train:0.767213 test:0.765157 best:0.780488 id:569\n",
      "[573/675] train:0.791314 test:0.780212 best:0.780488 id:569\n",
      "[574/675] train:0.724416 test:0.722628 best:0.780488 id:569\n",
      "[575/675] train:0.767437 test:0.765398 best:0.780488 id:569\n",
      "[576/675] train:0.791650 test:0.780192 best:0.780488 id:569\n",
      "[577/675] train:0.723603 test:0.721772 best:0.780488 id:569\n",
      "[578/675] train:0.767135 test:0.764895 best:0.780488 id:569\n",
      "[579/675] train:0.791214 test:0.780801 best:0.780801 id:578\n",
      "[580/675] train:0.723560 test:0.721610 best:0.780801 id:578\n",
      "[581/675] train:0.767372 test:0.765014 best:0.780801 id:578\n",
      "[582/675] train:0.791392 test:0.780618 best:0.780801 id:578\n",
      "[583/675] train:0.723602 test:0.721595 best:0.780801 id:578\n",
      "[584/675] train:0.767514 test:0.765102 best:0.780801 id:578\n",
      "[585/675] train:0.791560 test:0.780817 best:0.780817 id:584\n",
      "[586/675] train:0.723075 test:0.720734 best:0.780817 id:584\n",
      "[587/675] train:0.767591 test:0.764434 best:0.780817 id:584\n",
      "[588/675] train:0.790431 test:0.780892 best:0.780892 id:587\n",
      "[589/675] train:0.723056 test:0.720587 best:0.780892 id:587\n",
      "[590/675] train:0.767717 test:0.764571 best:0.780892 id:587\n",
      "[591/675] train:0.790465 test:0.780633 best:0.780892 id:587\n",
      "[592/675] train:0.723095 test:0.720611 best:0.780892 id:587\n",
      "[593/675] train:0.767820 test:0.764721 best:0.780892 id:587\n",
      "[594/675] train:0.790520 test:0.780628 best:0.780892 id:587\n",
      "[595/675] train:0.733203 test:0.730887 best:0.780892 id:587\n",
      "[596/675] train:0.774832 test:0.769299 best:0.780892 id:587\n",
      "[597/675] train:0.811842 test:0.778922 best:0.780892 id:587\n",
      "[598/675] train:0.733414 test:0.730654 best:0.780892 id:587\n",
      "[599/675] train:0.775492 test:0.769548 best:0.780892 id:587\n",
      "[600/675] train:0.813217 test:0.778859 best:0.780892 id:587\n",
      "[601/675] train:0.733794 test:0.730827 best:0.780892 id:587\n",
      "[602/675] train:0.775868 test:0.769784 best:0.780892 id:587\n",
      "[603/675] train:0.814219 test:0.778532 best:0.780892 id:587\n",
      "[604/675] train:0.732740 test:0.730149 best:0.780892 id:587\n",
      "[605/675] train:0.774829 test:0.768758 best:0.780892 id:587\n",
      "[606/675] train:0.812780 test:0.780134 best:0.780892 id:587\n",
      "[607/675] train:0.733266 test:0.730140 best:0.780892 id:587\n",
      "[608/675] train:0.775440 test:0.768979 best:0.780892 id:587\n",
      "[609/675] train:0.814188 test:0.779768 best:0.780892 id:587\n",
      "[610/675] train:0.733492 test:0.730121 best:0.780892 id:587\n",
      "[611/675] train:0.775848 test:0.769248 best:0.780892 id:587\n",
      "[612/675] train:0.815057 test:0.779838 best:0.780892 id:587\n",
      "[613/675] train:0.732953 test:0.729851 best:0.780892 id:587\n",
      "[614/675] train:0.774644 test:0.767751 best:0.780892 id:587\n",
      "[615/675] train:0.811798 test:0.780226 best:0.780892 id:587\n",
      "[616/675] train:0.733423 test:0.729519 best:0.780892 id:587\n",
      "[617/675] train:0.775151 test:0.767824 best:0.780892 id:587\n",
      "[618/675] train:0.812735 test:0.779883 best:0.780892 id:587\n",
      "[619/675] train:0.733535 test:0.729382 best:0.780892 id:587\n",
      "[620/675] train:0.775346 test:0.767902 best:0.780892 id:587\n",
      "[621/675] train:0.813027 test:0.779761 best:0.780892 id:587\n",
      "[622/675] train:0.742217 test:0.738815 best:0.780892 id:587\n",
      "[623/675] train:0.784869 test:0.772439 best:0.780892 id:587\n",
      "[624/675] train:0.849556 test:0.773619 best:0.780892 id:587\n",
      "[625/675] train:0.743253 test:0.739275 best:0.780892 id:587\n",
      "[626/675] train:0.786186 test:0.772707 best:0.780892 id:587\n",
      "[627/675] train:0.852362 test:0.772860 best:0.780892 id:587\n",
      "[628/675] train:0.744151 test:0.739718 best:0.780892 id:587\n",
      "[629/675] train:0.786903 test:0.772764 best:0.780892 id:587\n",
      "[630/675] train:0.854641 test:0.773380 best:0.780892 id:587\n",
      "[631/675] train:0.741753 test:0.738058 best:0.780892 id:587\n",
      "[632/675] train:0.784934 test:0.772010 best:0.780892 id:587\n",
      "[633/675] train:0.851393 test:0.775650 best:0.780892 id:587\n",
      "[634/675] train:0.743101 test:0.738598 best:0.780892 id:587\n",
      "[635/675] train:0.786182 test:0.772227 best:0.780892 id:587\n",
      "[636/675] train:0.854573 test:0.775299 best:0.780892 id:587\n",
      "[637/675] train:0.743959 test:0.738952 best:0.780892 id:587\n",
      "[638/675] train:0.786944 test:0.772251 best:0.780892 id:587\n",
      "[639/675] train:0.856919 test:0.774140 best:0.780892 id:587\n",
      "[640/675] train:0.741532 test:0.737224 best:0.780892 id:587\n",
      "[641/675] train:0.784482 test:0.770568 best:0.780892 id:587\n",
      "[642/675] train:0.851289 test:0.775778 best:0.780892 id:587\n",
      "[643/675] train:0.742940 test:0.737612 best:0.780892 id:587\n",
      "[644/675] train:0.785390 test:0.770497 best:0.780892 id:587\n",
      "[645/675] train:0.853505 test:0.775515 best:0.780892 id:587\n",
      "[646/675] train:0.743833 test:0.738036 best:0.780892 id:587\n",
      "[647/675] train:0.785745 test:0.770337 best:0.780892 id:587\n",
      "[648/675] train:0.855517 test:0.774556 best:0.780892 id:587\n",
      "[649/675] train:0.750900 test:0.745255 best:0.780892 id:587\n",
      "[650/675] train:0.799861 test:0.774624 best:0.780892 id:587\n",
      "[651/675] train:0.906525 test:0.765712 best:0.780892 id:587\n",
      "[652/675] train:0.751546 test:0.745198 best:0.780892 id:587\n",
      "[653/675] train:0.801935 test:0.774617 best:0.780892 id:587\n",
      "[654/675] train:0.912419 test:0.765187 best:0.780892 id:587\n",
      "[655/675] train:0.753022 test:0.745986 best:0.780892 id:587\n",
      "[656/675] train:0.803562 test:0.774648 best:0.780892 id:587\n",
      "[657/675] train:0.916172 test:0.763833 best:0.780892 id:587\n",
      "[658/675] train:0.750421 test:0.744331 best:0.780892 id:587\n",
      "[659/675] train:0.800367 test:0.774310 best:0.780892 id:587\n",
      "[660/675] train:0.910724 test:0.768561 best:0.780892 id:587\n",
      "[661/675] train:0.751432 test:0.744428 best:0.780892 id:587\n",
      "[662/675] train:0.802457 test:0.774518 best:0.780892 id:587\n",
      "[663/675] train:0.916220 test:0.767661 best:0.780892 id:587\n",
      "[664/675] train:0.753014 test:0.745222 best:0.780892 id:587\n",
      "[665/675] train:0.804048 test:0.774369 best:0.780892 id:587\n",
      "[666/675] train:0.919689 test:0.766869 best:0.780892 id:587\n",
      "[667/675] train:0.750195 test:0.743464 best:0.780892 id:587\n",
      "[668/675] train:0.799308 test:0.772521 best:0.780892 id:587\n",
      "[669/675] train:0.910134 test:0.769147 best:0.780892 id:587\n",
      "[670/675] train:0.751502 test:0.743530 best:0.780892 id:587\n",
      "[671/675] train:0.800819 test:0.772371 best:0.780892 id:587\n",
      "[672/675] train:0.914994 test:0.768739 best:0.780892 id:587\n",
      "[673/675] train:0.752734 test:0.743800 best:0.780892 id:587\n",
      "[674/675] train:0.801564 test:0.772024 best:0.780892 id:587\n",
      "[675/675] train:0.917849 test:0.766669 best:0.780892 id:587\n",
      "Wall time: 4h 32min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score, param, param_id = hyper_opt(XGBClassifier, xgb_param_grid, metrics.roc_auc_score,\n",
    "          X_train, y_train, X_val, y_val,\n",
    "          bin_prob=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 3000,\n",
       " 'max_depth': 2,\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': 1,\n",
       " 'colsample_bytree': 0.6,\n",
       " 'learning_rate': 0.1,\n",
       " 'tree_method': 'gpu_hist',\n",
       " 'evalmetric': 'auc'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_id = 587\n",
    "list(get_grid_iter(xgb_param_grid)[0])[587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
